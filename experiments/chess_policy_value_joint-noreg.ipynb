{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random \n",
    "import time\n",
    "from utils import Dataset,ProgressBar\n",
    "from tflearn.data_flow import DataFlow,DataFlowStatus,FeedDictFlow\n",
    "from tflearn.data_utils import Preloader,ImagePreloader\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import common\n",
    "import tflearn\n",
    "import copy\n",
    "from cchess import *\n",
    "from game_convert import convert_game,convert_game_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  6 01:33:17 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:05:00.0 Off |                  N/A |\r\n",
      "| 44%   74C    P2    86W / 250W |   6333MiB / 11172MiB |     31%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:42:00.0 Off |                  N/A |\r\n",
      "| 62%   86C    P2   190W / 250W |   4907MiB / 11172MiB |     98%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     17764      C   /home/meta/anaconda3/bin/python             5951MiB |\r\n",
      "|    0     19366      C   /usr/local/bin/python3                       183MiB |\r\n",
      "|    0     25225      C   /usr/local/bin/python3                       183MiB |\r\n",
      "|    1     17764      C   /home/meta/anaconda3/bin/python              153MiB |\r\n",
      "|    1     19366      C   /usr/local/bin/python3                      2369MiB |\r\n",
      "|    1     25225      C   /usr/local/bin/python3                      2369MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a network predict select and move of Chinese chess, with minimal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GPU_CORE = [1]\n",
    "BATCH_SIZE = 256\n",
    "BEGINING_LR = 0.01\n",
    "#TESTIMG_WIDTH = 500\n",
    "model_name = '5_5_resnet_joint_noreg'\n",
    "data_dir = 'data/imsa-cbf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = common.board.create_uci_labels()\n",
    "label2ind = dict(zip(labels,list(range(len(labels)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pgn2value = dict(pd.read_csv('./data/resultlist.csv').values[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ElePreloader(object):\n",
    "    def __init__(self,datafile,batch_size=64):\n",
    "        self.batch_size=batch_size\n",
    "        content = pd.read_csv(datafile,header=None,index_col=None)\n",
    "        self.filelist = [i[0] for i in content.get_values()]\n",
    "        self.pos = 0\n",
    "        self.feature_list = {\"red\":['A', 'B', 'C', 'K', 'N', 'P', 'R']\n",
    "                             ,\"black\":['a', 'b', 'c', 'k', 'n', 'p', 'r']}\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_iter = self.__iter()\n",
    "        assert(len(self.filelist) > batch_size)\n",
    "        self.game_iterlist = [None for i in self.filelist]\n",
    "    \n",
    "    def __iter(self):\n",
    "        retx1,rety1,retx2,rety2 = [],[],[],[]\n",
    "        vals = []\n",
    "        filelist = []\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.game_iterlist[i] == None:\n",
    "                    if len(filelist) == 0:\n",
    "                        filelist = copy.copy(self.filelist)\n",
    "                        random.shuffle(filelist)\n",
    "                    self.game_iterlist[i] = convert_game_value(filelist.pop(),self.feature_list,pgn2value)\n",
    "                game_iter = self.game_iterlist[i]\n",
    "                \n",
    "                try:\n",
    "                    x1,y1,val1 = game_iter.__next__()\n",
    "                    x1 = np.transpose(x1,[1,2,0])\n",
    "                    x1 = np.expand_dims(x1,axis=0)\n",
    "                    retx1.append(x1)\n",
    "                    #rety1.append(y1)\n",
    "                    oney = np.zeros(len(labels))\n",
    "                    oney[label2ind[''.join(y1)]] = 1\n",
    "                    rety1.append(oney)\n",
    "                    vals.append(val1)\n",
    "\n",
    "                    if len(retx1) >= self.batch_size:\n",
    "                        yield (np.concatenate(retx1,axis=0),np.asarray(rety1),np.asarray(vals))\n",
    "                        retx1,rety1 = [],[]\n",
    "                        vals = []\n",
    "                except :\n",
    "                    self.game_iterlist[i] = None\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        \n",
    "        x1,y1,val1 = self.batch_iter.__next__()\n",
    "        return x1,y1,val1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = ElePreloader(datafile='data/train_list.csv',batch_size=BATCH_SIZE)\n",
    "with tf.device(\"/gpu:{}\".format(GPU_CORE[0])):\n",
    "    coord = tf.train.Coordinator()\n",
    "    trainflow = FeedDictFlow({\n",
    "            'data':trainset,\n",
    "        },coord,batch_size=BATCH_SIZE,shuffle=True,continuous=True,num_threads=1)\n",
    "trainflow.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset = ElePreloader(datafile='data/test_list.csv',batch_size=BATCH_SIZE)\n",
    "with tf.device(\"/gpu:{}\".format(GPU_CORE[0])):\n",
    "    coord = tf.train.Coordinator()\n",
    "    testflow = FeedDictFlow({\n",
    "            'data':testset,\n",
    "        },coord,batch_size=BATCH_SIZE,shuffle=True,continuous=True,num_threads=1)\n",
    "testflow.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_x1,sample_y1,sample_value = trainflow.next()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_x1,sample_y1,sample_value = testflow.next()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 10, 9, 14), (256, 2086), (256,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x1.shape,sample_y1.shape,sample_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h2e2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(sample_y1[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=uint64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sample_x1[0],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  0., -1.,  0.,  1.,  0.,  1., -1.,  1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_value[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2086"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sample_x1[0],axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_block(inputx,name,training,block_num=2,filters=256,kernel_size=(3,3)):\n",
    "    net = inputx\n",
    "    for i in range(block_num):\n",
    "        net = tf.layers.conv2d(net,filters=filters,kernel_size=kernel_size,activation=None,name=\"{}_res_conv{}\".format(name,i),padding='same')\n",
    "        net = tf.layers.batch_normalization(net,training=training,name=\"{}_res_bn{}\".format(name,i))\n",
    "        if i == block_num - 1:\n",
    "            net = net + inputx #= tf.concat((inputx,net),axis=-1)\n",
    "        net = tf.nn.elu(net,name=\"{}_res_elu{}\".format(name,i))\n",
    "    return net\n",
    "\n",
    "def conv_block(inputx,name,training,block_num=1,filters=2,kernel_size=(1,1)):\n",
    "    net = inputx\n",
    "    for i in range(block_num):\n",
    "        net = tf.layers.conv2d(net,filters=filters,kernel_size=kernel_size,activation=None,name=\"{}_convblock_conv{}\".format(name,i),padding='same')\n",
    "        net = tf.layers.batch_normalization(net,training=training,name=\"{}_convblock_bn{}\".format(name,i))\n",
    "        net = tf.nn.elu(net,name=\"{}_convblock_elu{}\".format(name,i))\n",
    "    # net [None,10,9,2]\n",
    "    netshape = net.get_shape().as_list()\n",
    "    print(\"inside conv block {}\".format(str(netshape)))\n",
    "    net = tf.reshape(net,shape=(-1,netshape[1] * netshape[2] * netshape[3]))\n",
    "    net = tf.layers.dense(net,10 * 9,name=\"{}_dense\".format(name))\n",
    "    net = tf.nn.elu(net,name=\"{}_elu\".format(name))\n",
    "    return net\n",
    "\n",
    "def res_net_board(inputx,name,training,filters=256):\n",
    "    net = inputx\n",
    "    net = tf.layers.conv2d(net,filters=filters,kernel_size=(3,3),activation=None,name=\"{}_res_convb\".format(name),padding='same')\n",
    "    net = tf.layers.batch_normalization(net,training=training,name=\"{}_res_bnb\".format(name))\n",
    "    net = tf.nn.elu(net,name=\"{}_res_elub\".format(name))\n",
    "    for i in range(NUM_RES_LAYERS):\n",
    "        net = res_block(net,name=\"{}_layer_{}\".format(name,i + 1),training=training)\n",
    "        print(net.get_shape().as_list())\n",
    "    print(\"inside res net {}\".format(str(net.get_shape().as_list())))\n",
    "    #net_unsoftmax = conv_block(net,name=\"{}_conv\".format(name),training=training)\n",
    "    return net\n",
    "\n",
    "def get_scatter(name):\n",
    "    with tf.variable_scope(\"Test\"):\n",
    "        ph = tf.placeholder(tf.float32,name=name)\n",
    "        op = tf.summary.scalar(name,ph)\n",
    "    return ph,op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "    Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "      is over individual gradients. The inner list is over the gradient\n",
    "      calculation for each tower.\n",
    "    Returns:\n",
    "     List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "     across all towers.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(grads,0)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads\n",
    "\n",
    "def add_grad_to_list(opt,train_param,loss,tower_grad):\n",
    "    grads = opt.compute_gradients(loss, var_list = train_param)\n",
    "    grads = [i[0] for i in grads]\n",
    "    #print(grads)\n",
    "    tower_grad.append(zip(grads,train_param))\n",
    "    \n",
    "def get_op_mul(tower_gradients,optimizer,gs):\n",
    "    grads = average_gradients(tower_gradients)\n",
    "    train_op = optimizer.apply_gradients(grads,gs)\n",
    "    return train_op\n",
    "\n",
    "def reduce_mean(x):\n",
    "    return tf.reduce_mean(x)\n",
    "\n",
    "def merge(x):\n",
    "    return tf.concat(x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU_CORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "inside res net [None, 10, 9, 256]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "NUM_RES_LAYERS = 4\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "#with tf.device(\"/gpu:{}\".format(GPU_CORE)):\n",
    "    X = tf.placeholder(tf.float32,[None,10,9,14])\n",
    "    nextmove = tf.placeholder(tf.float32,[None,len(labels)])\n",
    "    score = tf.placeholder(tf.float32,[None,1])\n",
    "    \n",
    "    training = tf.placeholder(tf.bool,name='training_mode')\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    optimizer_policy = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "    optimizer_value = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "    optimizer_multitarg = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "    \n",
    "    tower_gradients_policy,tower_gradients_value,tower_gradients_multitarg = [],[],[]\n",
    "    \n",
    "    net_softmax_collection = []\n",
    "    value_head_collection = []\n",
    "    multitarget_loss_collection = []\n",
    "    value_loss_collection = []\n",
    "    policy_loss_collection = []\n",
    "    accuracy_select_collection = []\n",
    "    with tf.variable_scope(tf.get_variable_scope()) as vscope:\n",
    "        for ind,one_core in enumerate(GPU_CORE):\n",
    "            with tf.device(\"/gpu:{}\".format(one_core)):\n",
    "                print(ind)\n",
    "                body = res_net_board(X[ind * (BATCH_SIZE // len(GPU_CORE)):(ind + 1) * (BATCH_SIZE // len(GPU_CORE))],\n",
    "                                     \"selectnet\",training=training)\n",
    "                with tf.variable_scope(\"policy_head\"):\n",
    "                    policy_head = tf.layers.conv2d(body, 2, 1, padding='SAME')\n",
    "                    policy_head = tf.contrib.layers.batch_norm(policy_head, center=False, epsilon=1e-5, fused=True,\n",
    "                                                                is_training=training, activation_fn=tf.nn.relu)\n",
    "\n",
    "                    # print(self.policy_head.shape)  # (?, 9, 10, 2)\n",
    "                    policy_head = tf.reshape(policy_head, [-1, 9 * 10 * 2])\n",
    "                    policy_head = tf.contrib.layers.fully_connected(policy_head, len(labels), activation_fn=None)\n",
    "                    #self.policy_head.append(policy_head)    # 保存多个gpu的策略头结果（走子概率向量）\n",
    "\n",
    "                # 价值头\n",
    "                with tf.variable_scope(\"value_head\"):\n",
    "                    value_head = tf.layers.conv2d(body, 1, 1, padding='SAME')\n",
    "                    value_head = tf.contrib.layers.batch_norm(value_head, center=False, epsilon=1e-5, fused=True,\n",
    "                                                    is_training=training, activation_fn=tf.nn.relu)\n",
    "                    # print(self.value_head.shape)  # (?, 9, 10, 1)\n",
    "                    value_head = tf.reshape(value_head, [-1, 9 * 10 * 1])\n",
    "                    value_head = tf.contrib.layers.fully_connected(value_head, 256, activation_fn=tf.nn.relu)\n",
    "                    value_head = tf.contrib.layers.fully_connected(value_head, 1, activation_fn=tf.nn.tanh)\n",
    "                    value_head_collection.append(value_head)\n",
    "                net_unsoftmax = policy_head\n",
    "\n",
    "                with tf.variable_scope(\"Loss\"):\n",
    "                    policy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        labels=nextmove[ind * (BATCH_SIZE // len(GPU_CORE)):(ind + 1) * (BATCH_SIZE // len(GPU_CORE))],\n",
    "                        logits=net_unsoftmax))\n",
    "                    #loss_summary = tf.summary.scalar(\"move_loss\",policy_loss)\n",
    "                    value_loss = tf.losses.mean_squared_error(\n",
    "                        labels=score[ind * (BATCH_SIZE // len(GPU_CORE)):(ind + 1) * (BATCH_SIZE // len(GPU_CORE))],\n",
    "                        predictions=value_head) \n",
    "                    value_loss = tf.reduce_mean(value_loss)\n",
    "                    regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001)\n",
    "                    regular_variables = tf.trainable_variables()\n",
    "                    l2_loss = tf.contrib.layers.apply_regularization(regularizer, regular_variables)\n",
    "                    multitarget_loss = value_loss + policy_loss\n",
    "                    \n",
    "                    multitarget_loss_collection.append(multitarget_loss)\n",
    "                    value_loss_collection.append(value_loss)\n",
    "                    policy_loss_collection.append(policy_loss)\n",
    "                net_softmax = tf.nn.softmax(net_unsoftmax)\n",
    "                net_softmax_collection.append(net_softmax)\n",
    "                \n",
    "                correct_prediction = tf.equal(tf.argmax(nextmove,1), tf.argmax(net_softmax,1))\n",
    "\n",
    "                with tf.variable_scope(\"Accuracy\"):\n",
    "                    accuracy_select = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                    accuracy_select_collection.append(accuracy_select)\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                trainable_params = tf.trainable_variables()\n",
    "                tp_policy = [i for i in trainable_params if \n",
    "                                    ('value_head' not in i.name)]\n",
    "                tp_value = [i for i in trainable_params if \n",
    "                                    ('policy_head' not in i.name)]\n",
    "\n",
    "                add_grad_to_list(optimizer_policy,tp_policy,policy_loss,tower_gradients_policy)\n",
    "                add_grad_to_list(optimizer_value,tp_value,value_loss,tower_gradients_value)\n",
    "                add_grad_to_list(optimizer_multitarg,trainable_params,multitarget_loss,tower_gradients_multitarg)\n",
    "               \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        #gradients_policy = average_gradients(tower_gradients_policy)\n",
    "        train_op_policy = get_op_mul(tower_gradients_policy,optimizer_policy,global_step)\n",
    "        train_op_value = get_op_mul(tower_gradients_value,optimizer_value,global_step)\n",
    "        train_op_multitarg = get_op_mul(tower_gradients_multitarg,optimizer_multitarg,global_step)\n",
    "        #train_op = optimizer.minimize(policy_loss,global_step=global_step)\n",
    "    net_softmax = merge(net_softmax_collection)\n",
    "    value_head = merge(value_head_collection)\n",
    "    multitarget_loss = reduce_mean(multitarget_loss_collection)\n",
    "    value_loss = reduce_mean(value_loss_collection)\n",
    "    policy_loss = reduce_mean(policy_loss_collection)\n",
    "    accuracy_select = reduce_mean(accuracy_select_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.allow_soft_placement = True\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    tf.train.global_step(sess, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: 无法创建目录\"models\": 文件已存在\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"models/{}\".format(model_name)):\n",
    "    os.mkdir(\"models/{}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_BATCH = 10000 * (128 / BATCH_SIZE)\n",
    "N_BATCH_TEST = 300 * (128 / BATCH_SIZE)\n",
    "N_BATCH = int(N_BATCH)\n",
    "N_BATCH_TEST = int(N_BATCH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 150)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_BATCH,N_BATCH_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 256)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind * (BATCH_SIZE // len(GPU_CORE)),(ind + 1) * (BATCH_SIZE // len(GPU_CORE))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with graph.as_default():\n",
    "    train_epoch = 32\n",
    "    train_batch = 0\n",
    "    saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "    saver.restore(sess,\"models/{}/model_{}\".format(model_name,train_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2 STEP 4999 LR 0.1 ACC 26.05 LOSS 2.89 value_loss 0.55 100.00 % [==================================================>] 1280000/1280000 \t used:1866s eta:0 sssed:0s eta:1843 s\n",
      " epoch 2 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 2 valid loss 2.917412757873535 value loss 0.58990877866745 acc 25.0709375\n",
      "\n",
      "EPOCH 3 STEP 4999 LR 0.1 ACC 30.65 LOSS 2.63 value_loss 0.55 100.00 % [==================================================>] 1280000/1280000 \t used:1881s eta:0 ss\n",
      " epoch 3 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 3 valid loss 2.7014822959899902 value loss 0.5776419043540955 acc 29.2575\n",
      "\n",
      "EPOCH 4 STEP 4999 LR 0.1 ACC 31.14 LOSS 2.58 value_loss 0.55 100.00 % [==================================================>] 1280000/1280000 \t used:1880s eta:0 ss\n",
      " epoch 4 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 4 valid loss 2.587031364440918 value loss 0.5742527842521667 acc 31.0484375\n",
      "\n",
      "EPOCH 5 STEP 4999 LR 0.1 ACC 34.63 LOSS 2.41 value_loss 0.48 100.00 % [==================================================>] 1280000/1280000 \t used:1881s eta:0 ss\n",
      " epoch 5 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 5 valid loss 2.488619804382324 value loss 0.5851399898529053 acc 32.814375\n",
      "\n",
      "EPOCH 6 STEP 4999 LR 0.1 ACC 34.51 LOSS 2.4 value_loss 0.52 100.00 % [==================================================>] 1280000/1280000 \t used:1882s eta:0 sss\n",
      " epoch 6 100.00 % [==================================================>] 320000/320000 \t used:206s eta:0 sEPOCH 6 valid loss 2.4081578254699707 value loss 0.5772756934165955 acc 34.399375\n",
      "\n",
      "EPOCH 7 STEP 4999 LR 0.1 ACC 35.93 LOSS 2.33 value_loss 0.51 100.00 % [==================================================>] 1280000/1280000 \t used:1820s eta:0 ss\n",
      " epoch 7 100.00 % [==================================================>] 320000/320000 \t used:206s eta:0 sEPOCH 7 valid loss 2.3418262004852295 value loss 0.5486124753952026 acc 35.749375\n",
      "\n",
      "EPOCH 8 STEP 4999 LR 0.1 ACC 37.34 LOSS 2.25 value_loss 0.5 100.00 % [==================================================>] 1280000/1280000 \t used:1808s eta:0 sss\n",
      " epoch 8 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 8 valid loss 2.307574510574341 value loss 0.5471314191818237 acc 36.5646875\n",
      "\n",
      "EPOCH 9 STEP 4999 LR 0.1 ACC 39.09 LOSS 2.19 value_loss 0.51 100.00 % [==================================================>] 1280000/1280000 \t used:1882s eta:0 ss\n",
      " epoch 9 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 9 valid loss 2.2630350589752197 value loss 0.5486271381378174 acc 37.3090625\n",
      "\n",
      "EPOCH 10 STEP 4999 LR 0.1 ACC 38.5 LOSS 2.17 value_loss 0.51 100.00 % [==================================================>] 1280000/1280000 \t used:1900s eta:0 sss\n",
      " epoch 10 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 10 valid loss 2.2056543827056885 value loss 0.5540016889572144 acc 38.14875\n",
      "\n",
      "EPOCH 11 STEP 4999 LR 0.1 ACC 40.65 LOSS 2.07 value_loss 0.49 100.00 % [==================================================>] 1280000/1280000 \t used:1899s eta:0 ss\n",
      " epoch 11 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 11 valid loss 2.1856529712677 value loss 0.5475249290466309 acc 38.711875\n",
      "\n",
      "EPOCH 12 STEP 4999 LR 0.1 ACC 41.37 LOSS 2.05 value_loss 0.45 100.00 % [==================================================>] 1280000/1280000 \t used:1882s eta:0 ss\n",
      " epoch 12 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 12 valid loss 2.1585240364074707 value loss 0.5463425517082214 acc 39.1759375\n",
      "\n",
      "EPOCH 13 STEP 4999 LR 0.1 ACC 41.43 LOSS 2.04 value_loss 0.5 100.00 % [==================================================>] 1280000/1280000 \t used:1887s eta:0 sss\n",
      " epoch 13 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 13 valid loss 2.1309046745300293 value loss 0.5592357516288757 acc 39.7915625\n",
      "\n",
      "EPOCH 14 STEP 4999 LR 0.1 ACC 42.48 LOSS 1.99 value_loss 0.46 100.00 % [==================================================>] 1280000/1280000 \t used:1884s eta:0 ss\n",
      " epoch 14 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 14 valid loss 2.111769437789917 value loss 0.5552213191986084 acc 40.134375\n",
      "\n",
      "EPOCH 15 STEP 4999 LR 0.05 ACC 44.91 LOSS 1.89 value_loss 0.45 100.00 % [==================================================>] 1280000/1280000 \t used:1887s eta:0 ss\n",
      " epoch 15 100.00 % [==================================================>] 320000/320000 \t used:206s eta:0 sEPOCH 15 valid loss 2.021864175796509 value loss 0.5459371209144592 acc 41.944375\n",
      "\n",
      "EPOCH 16 STEP 4999 LR 0.05 ACC 44.51 LOSS 1.9 value_loss 0.46 100.00 % [==================================================>] 1280000/1280000 \t used:1887s eta:0 sss\n",
      " epoch 16 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 16 valid loss 2.0137414932250977 value loss 0.5400486588478088 acc 42.2828125\n",
      "\n",
      "EPOCH 17 STEP 4999 LR 0.05 ACC 45.6 LOSS 1.83 value_loss 0.43 100.00 % [==================================================>] 1280000/1280000 \t used:1885s eta:0 sss\n",
      " epoch 17 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 17 valid loss 2.016819477081299 value loss 0.5291798710823059 acc 42.308125\n",
      "\n",
      "EPOCH 18 STEP 4999 LR 0.05 ACC 46.35 LOSS 1.83 value_loss 0.44 100.00 % [==================================================>] 1280000/1280000 \t used:2095s eta:0 ss\n",
      " epoch 18 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 18 valid loss 2.0071375370025635 value loss 0.5661276578903198 acc 42.30375\n",
      "\n",
      "EPOCH 19 STEP 4999 LR 0.05 ACC 45.81 LOSS 1.85 value_loss 0.48 100.00 % [==================================================>] 1280000/1280000 \t used:2649s eta:0 ss\n",
      " epoch 19 100.00 % [==================================================>] 320000/320000 \t used:211s eta:0 sEPOCH 19 valid loss 1.988633155822754 value loss 0.5309765934944153 acc 42.689375\n",
      "\n",
      "EPOCH 20 STEP 4999 LR 0.05 ACC 48.2 LOSS 1.75 value_loss 0.43 100.00 % [==================================================>] 1280000/1280000 \t used:2671s eta:0 sss\n",
      " epoch 20 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 20 valid loss 1.9948004484176636 value loss 0.5291399359703064 acc 42.8325\n",
      "\n",
      "EPOCH 21 STEP 4999 LR 0.05 ACC 47.24 LOSS 1.78 value_loss 0.47 100.00 % [==================================================>] 1280000/1280000 \t used:2468s eta:0 ss\n",
      " epoch 21 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 21 valid loss 1.9844683408737183 value loss 0.5241886973381042 acc 42.93\n",
      "\n",
      "EPOCH 22 STEP 4999 LR 0.05 ACC 47.67 LOSS 1.77 value_loss 0.47 100.00 % [==================================================>] 1280000/1280000 \t used:1910s eta:0 ss\n",
      " epoch 22 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 22 valid loss 1.9708704948425293 value loss 0.5533598065376282 acc 43.1\n",
      "\n",
      "EPOCH 23 STEP 4999 LR 0.05 ACC 49.25 LOSS 1.7 value_loss 0.4 100.00 % [==================================================>] 1280000/1280000 \t used:1668s eta:0 ssss\n",
      " epoch 23 100.00 % [==================================================>] 320000/320000 \t used:206s eta:0 sEPOCH 23 valid loss 1.9789011478424072 value loss 0.5478041768074036 acc 43.1075\n",
      "\n",
      "EPOCH 24 STEP 4999 LR 0.05 ACC 48.13 LOSS 1.75 value_loss 0.42 100.00 % [==================================================>] 1280000/1280000 \t used:1168s eta:0 ss\n",
      " epoch 24 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 24 valid loss 1.9727363586425781 value loss 0.5455754995346069 acc 43.48125\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 25 STEP 4999 LR 0.05 ACC 48.08 LOSS 1.75 value_loss 0.42 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 ss\n",
      " epoch 25 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 25 valid loss 1.959934949874878 value loss 0.5295965075492859 acc 43.7546875\n",
      "\n",
      "EPOCH 26 STEP 4999 LR 0.05 ACC 51.66 LOSS 1.61 value_loss 0.41 100.00 % [==================================================>] 1280000/1280000 \t used:1165s eta:0 ss\n",
      " epoch 26 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 26 valid loss 1.97722327709198 value loss 0.5427936315536499 acc 43.22125\n",
      "\n",
      "EPOCH 27 STEP 4999 LR 0.05 ACC 49.43 LOSS 1.7 value_loss 0.42 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 sss\n",
      " epoch 27 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 27 valid loss 1.9750562906265259 value loss 0.5333890914916992 acc 43.7590625\n",
      "\n",
      "EPOCH 28 STEP 4999 LR 0.05 ACC 49.07 LOSS 1.71 value_loss 0.45 100.00 % [==================================================>] 1280000/1280000 \t used:1170s eta:0 ss\n",
      " epoch 28 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 28 valid loss 1.9621949195861816 value loss 0.5301185846328735 acc 43.525625\n",
      "\n",
      "EPOCH 29 STEP 4999 LR 0.05 ACC 53.41 LOSS 1.54 value_loss 0.39 100.00 % [==================================================>] 1280000/1280000 \t used:1168s eta:0 ss\n",
      " epoch 29 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 29 valid loss 1.969646692276001 value loss 0.5190041065216064 acc 43.8446875\n",
      "\n",
      "EPOCH 30 STEP 4999 LR 0.025 ACC 53.33 LOSS 1.54 value_loss 0.38 100.00 % [==================================================>] 1280000/1280000 \t used:1166s eta:0 ss\n",
      " epoch 30 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 30 valid loss 1.9263542890548706 value loss 0.5325778722763062 acc 44.9975\n",
      "\n",
      "EPOCH 31 STEP 4999 LR 0.025 ACC 54.01 LOSS 1.54 value_loss 0.42 100.00 % [==================================================>] 1280000/1280000 \t used:1173s eta:0 ss\n",
      " epoch 31 100.00 % [==================================================>] 320000/320000 \t used:206s eta:0 sEPOCH 31 valid loss 1.930911660194397 value loss 0.5260154008865356 acc 44.9790625\n",
      "\n",
      "EPOCH 32 STEP 4999 LR 0.025 ACC 52.8 LOSS 1.58 value_loss 0.41 100.00 % [==================================================>] 1280000/1280000 \t used:1169s eta:0 sss\n",
      " epoch 32 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 32 valid loss 1.9240573644638062 value loss 0.525362491607666 acc 45.079375\n",
      "\n",
      "EPOCH 33 STEP 4999 LR 0.025 ACC 55.04 LOSS 1.48 value_loss 0.38 100.00 % [==================================================>] 1280000/1280000 \t used:1164s eta:0 ss\n",
      " epoch 33 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 33 valid loss 1.9432721138000488 value loss 0.5311855673789978 acc 45.05\n",
      "\n",
      "EPOCH 34 STEP 4999 LR 0.025 ACC 55.02 LOSS 1.51 value_loss 0.43 100.00 % [==================================================>] 1280000/1280000 \t used:1164s eta:0 ss\n",
      " epoch 34 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 34 valid loss 1.9453908205032349 value loss 0.5328925848007202 acc 44.8228125\n",
      "\n",
      "EPOCH 35 STEP 4999 LR 0.025 ACC 53.05 LOSS 1.56 value_loss 0.39 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 ss\n",
      " epoch 35 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 35 valid loss 1.943243384361267 value loss 0.5320773720741272 acc 45.0315625\n",
      "\n",
      "EPOCH 36 STEP 4999 LR 0.025 ACC 56.68 LOSS 1.42 value_loss 0.36 100.00 % [==================================================>] 1280000/1280000 \t used:1162s eta:0 ss\n",
      " epoch 36 100.00 % [==================================================>] 320000/320000 \t used:206s eta:0 sEPOCH 36 valid loss 1.9580316543579102 value loss 0.5432581901550293 acc 44.9771875\n",
      "\n",
      "EPOCH 37 STEP 4999 LR 0.025 ACC 55.34 LOSS 1.47 value_loss 0.38 100.00 % [==================================================>] 1280000/1280000 \t used:1164s eta:0 ss\n",
      " epoch 37 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 37 valid loss 1.9459574222564697 value loss 0.5181837677955627 acc 45.124375\n",
      "\n",
      "EPOCH 38 STEP 4999 LR 0.025 ACC 54.93 LOSS 1.5 value_loss 0.38 100.00 % [==================================================>] 1280000/1280000 \t used:1160s eta:0 sss\n",
      " epoch 38 100.00 % [==================================================>] 320000/320000 \t used:206s eta:0 sEPOCH 38 valid loss 1.95082426071167 value loss 0.5475838780403137 acc 45.09375\n",
      "\n",
      "EPOCH 39 STEP 4999 LR 0.025 ACC 58.62 LOSS 1.36 value_loss 0.36 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 ss\n",
      " epoch 39 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 39 valid loss 1.9734758138656616 value loss 0.5265491604804993 acc 45.1015625\n",
      "\n",
      "EPOCH 40 STEP 4999 LR 0.025 ACC 55.76 LOSS 1.45 value_loss 0.38 100.00 % [==================================================>] 1280000/1280000 \t used:1165s eta:0 ss\n",
      " epoch 40 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 40 valid loss 1.9664019346237183 value loss 0.5411527752876282 acc 45.373125\n",
      "\n",
      "EPOCH 41 STEP 4999 LR 0.025 ACC 56.16 LOSS 1.44 value_loss 0.37 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 ss\n",
      " epoch 41 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 41 valid loss 1.9599202871322632 value loss 0.5327379703521729 acc 45.3034375\n",
      "\n",
      "EPOCH 42 STEP 4999 LR 0.025 ACC 58.22 LOSS 1.38 value_loss 0.37 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 ss\n",
      " epoch 42 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 42 valid loss 2.0001540184020996 value loss 0.5641260743141174 acc 44.8040625\n",
      "\n",
      "EPOCH 43 STEP 4999 LR 0.025 ACC 56.94 LOSS 1.43 value_loss 0.38 100.00 % [==================================================>] 1280000/1280000 \t used:1168s eta:0 ss\n",
      " epoch 43 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 43 valid loss 1.9820138216018677 value loss 0.537164032459259 acc 45.3296875\n",
      "\n",
      "EPOCH 44 STEP 4999 LR 0.025 ACC 57.03 LOSS 1.42 value_loss 0.38 100.00 % [==================================================>] 1280000/1280000 \t used:1170s eta:0 ss\n",
      " epoch 44 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 44 valid loss 1.974702000617981 value loss 0.5286250114440918 acc 45.3946875\n",
      "\n",
      "EPOCH 45 STEP 4999 LR 0.0125 ACC 61.59 LOSS 1.26 value_loss 0.34 100.00 % [==================================================>] 1280000/1280000 \t used:1166s eta:0 ss\n",
      " epoch 45 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 45 valid loss 1.9868378639221191 value loss 0.5404730439186096 acc 45.749375\n",
      "\n",
      "EPOCH 46 STEP 4999 LR 0.0125 ACC 61.56 LOSS 1.27 value_loss 0.34 100.00 % [==================================================>] 1280000/1280000 \t used:1165s eta:0 ss\n",
      " epoch 46 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 46 valid loss 2.003242015838623 value loss 0.546577513217926 acc 45.7265625\n",
      "\n",
      "EPOCH 47 STEP 4999 LR 0.0125 ACC 59.75 LOSS 1.32 value_loss 0.36 100.00 % [==================================================>] 1280000/1280000 \t used:1174s eta:0 ss\n",
      " epoch 47 100.00 % [==================================================>] 320000/320000 \t used:206s eta:0 sEPOCH 47 valid loss 1.994106650352478 value loss 0.5378978252410889 acc 45.89625\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 48 STEP 4999 LR 0.0125 ACC 63.8 LOSS 1.19 value_loss 0.32 100.00 % [==================================================>] 1280000/1280000 \t used:1182s eta:0 sss\n",
      " epoch 48 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 48 valid loss 2.014369010925293 value loss 0.5324569940567017 acc 45.9296875\n",
      "\n",
      "EPOCH 49 STEP 4999 LR 0.0125 ACC 62.49 LOSS 1.25 value_loss 0.36 100.00 % [==================================================>] 1280000/1280000 \t used:1192s eta:0 ss\n",
      " epoch 49 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 49 valid loss 2.019029140472412 value loss 0.5477239489555359 acc 45.941875\n",
      "\n",
      "EPOCH 50 STEP 4999 LR 0.0125 ACC 61.27 LOSS 1.28 value_loss 0.35 100.00 % [==================================================>] 1280000/1280000 \t used:1205s eta:0 ss\n",
      " epoch 50 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 50 valid loss 2.0057127475738525 value loss 0.5470867156982422 acc 46.0746875\n",
      "\n",
      "EPOCH 51 STEP 4999 LR 0.0125 ACC 64.75 LOSS 1.16 value_loss 0.34 100.00 % [==================================================>] 1280000/1280000 \t used:1219s eta:0 ss\n",
      " epoch 51 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 51 valid loss 2.039297342300415 value loss 0.5528371930122375 acc 45.6684375\n",
      "\n",
      "EPOCH 52 STEP 4999 LR 0.0125 ACC 63.14 LOSS 1.21 value_loss 0.33 100.00 % [==================================================>] 1280000/1280000 \t used:1231s eta:0 ss\n",
      " epoch 52 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 52 valid loss 2.0357749462127686 value loss 0.5485738515853882 acc 45.963125\n",
      "\n",
      "EPOCH 53 STEP 4999 LR 0.0125 ACC 62.23 LOSS 1.26 value_loss 0.32 100.00 % [==================================================>] 1280000/1280000 \t used:1250s eta:0 ss\n",
      " epoch 53 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 53 valid loss 2.0422863960266113 value loss 0.5522645711898804 acc 45.858125\n",
      "\n",
      "EPOCH 54 STEP 4999 LR 0.0125 ACC 64.74 LOSS 1.16 value_loss 0.33 100.00 % [==================================================>] 1280000/1280000 \t used:1266s eta:0 ss\n",
      " epoch 54 100.00 % [==================================================>] 320000/320000 \t used:210s eta:0 sEPOCH 54 valid loss 2.0335323810577393 value loss 0.5414710640907288 acc 46.1234375\n",
      "\n",
      "EPOCH 55 STEP 4999 LR 0.0125 ACC 63.54 LOSS 1.2 value_loss 0.32 100.00 % [==================================================>] 1280000/1280000 \t used:1265s eta:0 sss\n",
      " epoch 55 100.00 % [==================================================>] 320000/320000 \t used:212s eta:0 sEPOCH 55 valid loss 2.071526527404785 value loss 0.5590335726737976 acc 45.775\n",
      "\n",
      "EPOCH 56 STEP 4999 LR 0.0125 ACC 62.55 LOSS 1.23 value_loss 0.34 100.00 % [==================================================>] 1280000/1280000 \t used:1240s eta:0 ss\n",
      " epoch 56 100.00 % [==================================================>] 320000/320000 \t used:213s eta:0 sEPOCH 56 valid loss 2.0626299381256104 value loss 0.5439887642860413 acc 45.6746875\n",
      "\n",
      "EPOCH 57 STEP 4999 LR 0.0125 ACC 65.77 LOSS 1.14 value_loss 0.32 100.00 % [==================================================>] 1280000/1280000 \t used:1222s eta:0 ss\n",
      " epoch 57 100.00 % [==================================================>] 320000/320000 \t used:214s eta:0 sEPOCH 57 valid loss 2.0490996837615967 value loss 0.5539605617523193 acc 46.1509375\n",
      "\n",
      "EPOCH 58 STEP 4999 LR 0.0125 ACC 64.52 LOSS 1.17 value_loss 0.34 100.00 % [==================================================>] 1280000/1280000 \t used:1204s eta:0 ss\n",
      " epoch 58 100.00 % [==================================================>] 320000/320000 \t used:215s eta:0 sEPOCH 58 valid loss 2.0979065895080566 value loss 0.5494295954704285 acc 45.7403125\n",
      "\n",
      "EPOCH 59 STEP 4999 LR 0.0125 ACC 63.91 LOSS 1.19 value_loss 0.34 100.00 % [==================================================>] 1280000/1280000 \t used:1190s eta:0 ss\n",
      " epoch 59 100.00 % [==================================================>] 320000/320000 \t used:215s eta:0 sEPOCH 59 valid loss 2.071550130844116 value loss 0.5478208661079407 acc 46.22\n",
      "\n",
      "EPOCH 60 STEP 4999 LR 0.00625 ACC 65.38 LOSS 1.13 value_loss 0.34 100.00 % [==================================================>] 1280000/1280000 \t used:1178s eta:0 ss\n",
      " epoch 60 100.00 % [==================================================>] 320000/320000 \t used:214s eta:0 sEPOCH 60 valid loss 2.0694785118103027 value loss 0.5450847744941711 acc 46.455625\n",
      "\n",
      "EPOCH 61 STEP 4999 LR 0.00625 ACC 68.3 LOSS 1.06 value_loss 0.31 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 sss\n",
      " epoch 61 100.00 % [==================================================>] 320000/320000 \t used:215s eta:0 sEPOCH 61 valid loss 2.095520496368408 value loss 0.547993540763855 acc 46.7103125\n",
      "\n",
      "EPOCH 62 STEP 4999 LR 0.00625 ACC 67.13 LOSS 1.09 value_loss 0.33 100.00 % [==================================================>] 1280000/1280000 \t used:1165s eta:0 ss\n",
      " epoch 62 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 62 valid loss 2.1142797470092773 value loss 0.5543500185012817 acc 46.2665625\n",
      "\n",
      "EPOCH 63 STEP 4999 LR 0.00625 ACC 66.62 LOSS 1.1 value_loss 0.32 100.00 % [==================================================>] 1280000/1280000 \t used:1168s eta:0 sss\n",
      " epoch 63 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 63 valid loss 2.100857973098755 value loss 0.5471752882003784 acc 46.4403125\n",
      "\n",
      "EPOCH 64 STEP 4999 LR 0.00625 ACC 68.58 LOSS 1.04 value_loss 0.29 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 ss\n",
      " epoch 64 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 64 valid loss 2.139112949371338 value loss 0.5543193221092224 acc 46.1453125\n",
      "\n",
      "EPOCH 65 STEP 4999 LR 0.00625 ACC 67.54 LOSS 1.07 value_loss 0.33 100.00 % [==================================================>] 1280000/1280000 \t used:1168s eta:0 ss\n",
      " epoch 65 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 65 valid loss 2.145461320877075 value loss 0.5597314238548279 acc 46.25875\n",
      "\n",
      "EPOCH 66 STEP 4999 LR 0.00625 ACC 66.64 LOSS 1.09 value_loss 0.34 100.00 % [==================================================>] 1280000/1280000 \t used:1168s eta:0 ss\n",
      " epoch 66 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 66 valid loss 2.1160786151885986 value loss 0.5558530688285828 acc 46.68625\n",
      "\n",
      "EPOCH 67 STEP 4999 LR 0.00625 ACC 69.83 LOSS 1.01 value_loss 0.31 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 ss\n",
      " epoch 67 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 67 valid loss 2.1580970287323 value loss 0.5637491941452026 acc 46.40875\n",
      "\n",
      "EPOCH 68 STEP 4999 LR 0.00625 ACC 68.27 LOSS 1.05 value_loss 0.33 100.00 % [==================================================>] 1280000/1280000 \t used:1164s eta:0 ss\n",
      " epoch 68 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 68 valid loss 2.133122205734253 value loss 0.5618032217025757 acc 46.900625\n",
      "\n",
      "EPOCH 69 STEP 4999 LR 0.00625 ACC 66.12 LOSS 1.1 value_loss 0.32 100.00 % [==================================================>] 1280000/1280000 \t used:1165s eta:0 sss\n",
      " epoch 69 100.00 % [==================================================>] 320000/320000 \t used:206s eta:0 sEPOCH 69 valid loss 2.1660830974578857 value loss 0.5634649395942688 acc 46.1109375\n",
      "\n",
      "EPOCH 70 STEP 4999 LR 0.00625 ACC 69.15 LOSS 1.03 value_loss 0.29 100.00 % [==================================================>] 1280000/1280000 \t used:1164s eta:0 ss\n",
      " epoch 70 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 70 valid loss 2.17716908454895 value loss 0.5531474351882935 acc 46.2378125\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 71 STEP 4999 LR 0.00625 ACC 68.65 LOSS 1.03 value_loss 0.32 100.00 % [==================================================>] 1280000/1280000 \t used:1164s eta:0 ss\n",
      " epoch 71 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 71 valid loss 2.174715995788574 value loss 0.5648081302642822 acc 46.4453125\n",
      "\n",
      "EPOCH 72 STEP 4999 LR 0.00625 ACC 68.41 LOSS 1.03 value_loss 0.31 100.00 % [==================================================>] 1280000/1280000 \t used:1165s eta:0 ss\n",
      " epoch 72 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 72 valid loss 2.179154634475708 value loss 0.564125657081604 acc 46.0803125\n",
      "\n",
      "EPOCH 73 STEP 4999 LR 0.00625 ACC 70.65 LOSS 0.97 value_loss 0.3 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 sss\n",
      " epoch 73 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 73 valid loss 2.183903694152832 value loss 0.5652590990066528 acc 46.6465625\n",
      "\n",
      "EPOCH 74 STEP 4999 LR 0.00625 ACC 69.91 LOSS 1.0 value_loss 0.3 100.00 % [==================================================>] 1280000/1280000 \t used:1160s eta:0 ssss\n",
      " epoch 74 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 74 valid loss 2.194547176361084 value loss 0.5664807558059692 acc 46.466875\n",
      "\n",
      "EPOCH 75 STEP 4999 LR 0.003125 ACC 70.17 LOSS 0.99 value_loss 0.31 100.00 % [==================================================>] 1280000/1280000 \t used:1162s eta:0 ss\n",
      " epoch 75 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 75 valid loss 2.177891254425049 value loss 0.5518819093704224 acc 46.948125\n",
      "\n",
      "EPOCH 76 STEP 4999 LR 0.003125 ACC 71.76 LOSS 0.95 value_loss 0.28 100.00 % [==================================================>] 1280000/1280000 \t used:1160s eta:0 ss\n",
      " epoch 76 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 76 valid loss 2.2188737392425537 value loss 0.5810787081718445 acc 46.38125\n",
      "\n",
      "EPOCH 77 STEP 4999 LR 0.003125 ACC 71.24 LOSS 0.97 value_loss 0.31 100.00 % [==================================================>] 1280000/1280000 \t used:1163s eta:0 ss\n",
      " epoch 77 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 77 valid loss 2.215681314468384 value loss 0.5607305765151978 acc 46.886875\n",
      "\n",
      "EPOCH 78 STEP 4999 LR 0.003125 ACC 69.6 LOSS 1.0 value_loss 0.29 100.00 % [==================================================>] 1280000/1280000 \t used:1160s eta:0 ssss\n",
      " epoch 78 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 78 valid loss 2.213163375854492 value loss 0.5613557696342468 acc 46.6684375\n",
      "\n",
      "EPOCH 79 STEP 4999 LR 0.003125 ACC 73.05 LOSS 0.91 value_loss 0.3 100.00 % [==================================================>] 1280000/1280000 \t used:1164s eta:0 sss\n",
      " epoch 79 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 79 valid loss 2.2224984169006348 value loss 0.5561047792434692 acc 46.689375\n",
      "\n",
      "EPOCH 80 STEP 4999 LR 0.003125 ACC 71.78 LOSS 0.95 value_loss 0.31 100.00 % [==================================================>] 1280000/1280000 \t used:1164s eta:0 ss\n",
      " epoch 80 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 80 valid loss 2.2444074153900146 value loss 0.5897781252861023 acc 46.7184375\n",
      "\n",
      "EPOCH 81 STEP 4999 LR 0.003125 ACC 70.8 LOSS 0.97 value_loss 0.27 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 sss\n",
      " epoch 81 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 81 valid loss 2.2389633655548096 value loss 0.5563320517539978 acc 46.631875\n",
      "\n",
      "EPOCH 82 STEP 4999 LR 0.003125 ACC 73.02 LOSS 0.91 value_loss 0.28 100.00 % [==================================================>] 1280000/1280000 \t used:1166s eta:0 ss\n",
      " epoch 82 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 82 valid loss 2.227928400039673 value loss 0.5642637014389038 acc 46.8625\n",
      "\n",
      "EPOCH 83 STEP 4999 LR 0.003125 ACC 72.47 LOSS 0.92 value_loss 0.29 100.00 % [==================================================>] 1280000/1280000 \t used:1166s eta:0 ss\n",
      " epoch 83 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 83 valid loss 2.2605652809143066 value loss 0.5796305537223816 acc 46.6253125\n",
      "\n",
      "EPOCH 84 STEP 4999 LR 0.003125 ACC 72.67 LOSS 0.91 value_loss 0.28 100.00 % [==================================================>] 1280000/1280000 \t used:1163s eta:0 ss\n",
      " epoch 84 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 84 valid loss 2.2490031719207764 value loss 0.5671170949935913 acc 46.685625\n",
      "\n",
      "EPOCH 85 STEP 4999 LR 0.003125 ACC 71.72 LOSS 0.94 value_loss 0.29 100.00 % [==================================================>] 1280000/1280000 \t used:1161s eta:0 ss\n",
      " epoch 85 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 85 valid loss 2.260533094406128 value loss 0.5676312446594238 acc 46.59375\n",
      "\n",
      "EPOCH 86 STEP 4999 LR 0.003125 ACC 72.88 LOSS 0.91 value_loss 0.27 100.00 % [==================================================>] 1280000/1280000 \t used:1168s eta:0 ss\n",
      " epoch 86 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 86 valid loss 2.282043933868408 value loss 0.5834066271781921 acc 46.52375\n",
      "\n",
      "EPOCH 87 STEP 4999 LR 0.003125 ACC 72.12 LOSS 0.93 value_loss 0.29 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 ss\n",
      " epoch 87 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 87 valid loss 2.260049819946289 value loss 0.576070249080658 acc 46.9590625\n",
      "\n",
      "EPOCH 88 STEP 4999 LR 0.003125 ACC 71.59 LOSS 0.94 value_loss 0.28 100.00 % [==================================================>] 1280000/1280000 \t used:1166s eta:0 ss\n",
      " epoch 88 100.00 % [==================================================>] 320000/320000 \t used:207s eta:0 sEPOCH 88 valid loss 2.258941650390625 value loss 0.5613843202590942 acc 46.74125\n",
      "\n",
      "EPOCH 89 STEP 4999 LR 0.003125 ACC 73.03 LOSS 0.9 value_loss 0.28 100.00 % [==================================================>] 1280000/1280000 \t used:1167s eta:0 sss\n",
      " epoch 89 100.00 % [==================================================>] 320000/320000 \t used:208s eta:0 sEPOCH 89 valid loss 2.268160820007324 value loss 0.5784956812858582 acc 47.090625\n",
      "\n",
      "EPOCH 90 STEP 4999 LR 0.0015625 ACC 73.33 LOSS 0.9 value_loss 0.28 100.00 % [==================================================>] 1280000/1280000 \t used:1160s eta:0 sss\n",
      " epoch 90 100.00 % [==================================================>] 320000/320000 \t used:210s eta:0 sEPOCH 90 valid loss 2.304837942123413 value loss 0.5761906504631042 acc 46.4203125\n",
      "\n",
      "EPOCH 91 STEP 4999 LR 0.0015625 ACC 73.49 LOSS 0.9 value_loss 0.28 100.00 % [==================================================>] 1280000/1280000 \t used:1174s eta:0 sss\n",
      " epoch 91 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 91 valid loss 2.268299102783203 value loss 0.5794258713722229 acc 47.2665625\n",
      "\n",
      "EPOCH 92 STEP 4999 LR 0.0015625 ACC 74.55 LOSS 0.88 value_loss 0.29 100.00 % [==================================================>] 1280000/1280000 \t used:1193s eta:0 ss\n",
      " epoch 92 100.00 % [==================================================>] 320000/320000 \t used:210s eta:0 sEPOCH 92 valid loss 2.299374580383301 value loss 0.5734716057777405 acc 46.739375\n",
      "\n",
      "EPOCH 93 STEP 4999 LR 0.0015625 ACC 74.73 LOSS 0.85 value_loss 0.29 100.00 % [==================================================>] 1280000/1280000 \t used:1203s eta:0 ss\n",
      " epoch 93 100.00 % [==================================================>] 320000/320000 \t used:211s eta:0 sEPOCH 93 valid loss 2.3076446056365967 value loss 0.5803660750389099 acc 46.7478125\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 94 STEP 4999 LR 0.0015625 ACC 73.64 LOSS 0.88 value_loss 0.29 100.00 % [==================================================>] 1280000/1280000 \t used:1218s eta:0 ss\n",
      " epoch 94 100.00 % [==================================================>] 320000/320000 \t used:209s eta:0 sEPOCH 94 valid loss 2.298604726791382 value loss 0.5780712366104126 acc 46.9259375\n",
      "\n",
      "EPOCH 95 STEP 4999 LR 0.0015625 ACC 75.27 LOSS 0.85 value_loss 0.29 100.00 % [==================================================>] 1280000/1280000 \t used:1238s eta:0 ss\n",
      " epoch 95 100.00 % [==================================================>] 320000/320000 \t used:210s eta:0 sEPOCH 95 valid loss 2.3191332817077637 value loss 0.5732090473175049 acc 46.6421875\n",
      "\n",
      "EPOCH 96 STEP 4999 LR 0.0015625 ACC 73.57 LOSS 0.89 value_loss 0.28 100.00 % [==================================================>] 1280000/1280000 \t used:859s eta:0 ss\n",
      " epoch 96 100.00 % [==================================================>] 320000/320000 \t used:220s eta:0 sEPOCH 96 valid loss 2.305548906326294 value loss 0.5879277586936951 acc 47.0365625\n",
      "\n",
      "EPOCH 97 STEP 4999 LR 0.0015625 ACC 74.86 LOSS 0.87 value_loss 0.27 100.00 % [==================================================>] 1280000/1280000 \t used:859s eta:0 ss\n",
      " epoch 97 100.00 % [==================================================>] 320000/320000 \t used:220s eta:0 sEPOCH 97 valid loss 2.313072443008423 value loss 0.5878940224647522 acc 46.75375\n",
      "\n",
      "EPOCH 98 STEP 4999 LR 0.0015625 ACC 74.52 LOSS 0.87 value_loss 0.28 100.00 % [==================================================>] 1280000/1280000 \t used:861s eta:0 ss\n",
      " epoch 98 100.00 % [==================================================>] 320000/320000 \t used:220s eta:0 sEPOCH 98 valid loss 2.305979013442993 value loss 0.5627578496932983 acc 47.0771875\n",
      "\n",
      "EPOCH 99 STEP 4999 LR 0.0015625 ACC 74.86 LOSS 0.86 value_loss 0.29 100.00 % [==================================================>] 1280000/1280000 \t used:860s eta:0 ss\n",
      " epoch 99 100.00 % [==================================================>] 320000/320000 \t used:221s eta:0 sEPOCH 99 valid loss 2.333618402481079 value loss 0.5824258327484131 acc 46.8478125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "restore = False\n",
    "N_EPOCH = 100\n",
    "DECAY_EPOCH = 15\n",
    "\n",
    "class ExpVal:\n",
    "    def __init__(self,exp_a=0.97):\n",
    "        self.val = None\n",
    "        self.exp_a = exp_a\n",
    "    def update(self,newval):\n",
    "        if self.val == None:\n",
    "            self.val = newval\n",
    "        else:\n",
    "            self.val = self.exp_a * self.val + (1 - self.exp_a) * newval\n",
    "    def getval(self):\n",
    "        return round(self.val,2)\n",
    "    \n",
    "expacc_move = ExpVal()\n",
    "exploss = ExpVal()\n",
    "expsteploss = ExpVal()\n",
    "\n",
    "begining_learning_rate = 1e-1\n",
    "\n",
    "pred_image = None\n",
    "if restore == False:\n",
    "    train_epoch = 1\n",
    "    train_batch = 0\n",
    "for one_epoch in range(train_epoch + 1,N_EPOCH):\n",
    "    train_epoch = one_epoch\n",
    "    pb = ProgressBar(worksum=N_BATCH * BATCH_SIZE,info=\" epoch {} batch {}\".format(train_epoch,train_batch))\n",
    "    pb.startjob()\n",
    "    \n",
    "    for one_batch in range(N_BATCH):\n",
    "        if restore == True and one_batch < train_batch:\n",
    "            pb.auto_display = False\n",
    "            pb.complete(BATCH_SIZE)\n",
    "            pb.auto_display = True\n",
    "            continue\n",
    "        else:\n",
    "            restore = False\n",
    "        train_batch = one_batch\n",
    "        \n",
    "        batch_x,batch_y,batch_v = trainflow.next()['data']\n",
    "        batch_v = np.expand_dims(np.nan_to_num(batch_v),1)\n",
    "        # learning rate decay strategy\n",
    "        batch_lr = begining_learning_rate * 2 ** -(one_epoch // DECAY_EPOCH)\n",
    "        with graph.as_default():\n",
    "            _,step_loss,step_value_loss,step_acc_move,step_value,step_val_predict = sess.run(\n",
    "                [train_op_multitarg,policy_loss,value_loss,accuracy_select,global_step,value_head],feed_dict={\n",
    "                    X:batch_x,nextmove:batch_y,learning_rate:batch_lr,training:True,score:batch_v,\n",
    "                })\n",
    "        \n",
    "        step_acc_move *= 100\n",
    "        \n",
    "        expacc_move.update(step_acc_move)\n",
    "        exploss.update(step_loss)\n",
    "        expsteploss.update(step_value_loss)\n",
    "\n",
    "       \n",
    "        pb.info = \"EPOCH {} STEP {} LR {} ACC {} LOSS {} value_loss {}\".format(\n",
    "            one_epoch,one_batch,batch_lr,expacc_move.getval(),exploss.getval(),expsteploss.getval())\n",
    "        \n",
    "        pb.complete(BATCH_SIZE)\n",
    "    print()\n",
    "    pb = ProgressBar(worksum=N_BATCH // 4 * BATCH_SIZE,info=\" epoch {}\".format(train_epoch))\n",
    "    pb.startjob()\n",
    "    losses = []\n",
    "    value_losses = []\n",
    "    accs = []\n",
    "    for one_batch in range(N_BATCH // 4):\n",
    "        batch_x,batch_y,batch_v = testflow.next()['data']\n",
    "        batch_v = np.expand_dims(np.nan_to_num(batch_v),1)\n",
    "        # learning rate decay strategy\n",
    "        batch_lr = begining_learning_rate * 10 ** -(one_epoch // DECAY_EPOCH)\n",
    "        with graph.as_default():\n",
    "            step_loss,step_value_loss,step_acc_move,step_value = sess.run(\n",
    "                [policy_loss,value_loss,accuracy_select,global_step],feed_dict={\n",
    "                    X:batch_x,nextmove:batch_y,learning_rate:batch_lr,training:False,score:batch_v,\n",
    "                })\n",
    "        \n",
    "        step_acc_move *= 100\n",
    "        losses.append(step_loss)\n",
    "        accs.append(step_acc_move)\n",
    "        value_losses.append(step_value_loss)\n",
    "        pb.complete(BATCH_SIZE)\n",
    "    print(\"EPOCH {} valid loss {} value loss {} acc {}\".format(train_epoch,np.average(losses)\n",
    "                                                               ,np.average(value_losses),np.average(accs)))\n",
    "    print()\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "        saver.save(sess,\"models/{}/model_{}\".format(model_name,one_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tf1.3_python",
   "language": "python",
   "name": "tf1.3_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
