{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random \n",
    "import time\n",
    "from utils import Dataset,ProgressBar\n",
    "from tflearn.data_flow import DataFlow,DataFlowStatus,FeedDictFlow\n",
    "from tflearn.data_utils import Preloader,ImagePreloader\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import common\n",
    "import tflearn\n",
    "import copy\n",
    "from cchess import *\n",
    "from game_convert import convert_game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a network predict select and move of Chinese chess, with minimal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GPU_CORE = 0\n",
    "BATCH_SIZE = 128\n",
    "BEGINING_LR = 0.01\n",
    "#TESTIMG_WIDTH = 500\n",
    "model_name = '11_10_resnet20_shallow5'\n",
    "data_dir = 'data/imsa-cbf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = common.board.create_uci_labels()\n",
    "label2ind = dict(zip(labels,list(range(len(labels)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ElePreloader(object):\n",
    "    def __init__(self,datafile,batch_size=64):\n",
    "        self.batch_size=batch_size\n",
    "        content = pd.read_csv(datafile,header=None,index_col=None)\n",
    "        self.filelist = [i[0] for i in content.get_values()]\n",
    "        self.pos = 0\n",
    "        self.feature_list = {\"red\":['A', 'B', 'C', 'K', 'N', 'P', 'R']\n",
    "                             ,\"black\":['a', 'b', 'c', 'k', 'n', 'p', 'r']}\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_iter = self.__iter()\n",
    "        assert(len(self.filelist) > batch_size)\n",
    "        self.game_iterlist = [None for i in self.filelist]\n",
    "    \n",
    "    def __iter(self):\n",
    "        retx1,rety1,retx2,rety2 = [],[],[],[]\n",
    "        filelist = []\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.game_iterlist[i] == None:\n",
    "                    if len(filelist) == 0:\n",
    "                        filelist = copy.copy(self.filelist)\n",
    "                        random.shuffle(filelist)\n",
    "                    self.game_iterlist[i] = convert_game(filelist.pop(),feature_list=self.feature_list)\n",
    "                game_iter = self.game_iterlist[i]\n",
    "                \n",
    "                try:\n",
    "                    x1,y1 = game_iter.__next__()\n",
    "                    x1 = np.transpose(x1,[1,2,0])\n",
    "                    x1 = np.expand_dims(x1,axis=0)\n",
    "                    retx1.append(x1)\n",
    "                    #rety1.append(y1)\n",
    "                    oney = np.zeros(len(labels))\n",
    "                    oney[label2ind[''.join(y1)]] = 1\n",
    "                    rety1.append(oney)\n",
    "\n",
    "                    if len(retx1) >= self.batch_size:\n",
    "                        yield (np.concatenate(retx1,axis=0),np.asarray(rety1))\n",
    "                        retx1,rety1 = [],[]\n",
    "                except :\n",
    "                    self.game_iterlist[i] = None\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        \n",
    "        x1,y1 = self.batch_iter.__next__()\n",
    "        return x1,y1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = ElePreloader(datafile='data/train_list.csv',batch_size=BATCH_SIZE)\n",
    "with tf.device(\"/gpu:{}\".format(GPU_CORE)):\n",
    "    coord = tf.train.Coordinator()\n",
    "    trainflow = FeedDictFlow({\n",
    "            'data':trainset,\n",
    "        },coord,batch_size=BATCH_SIZE,shuffle=True,continuous=True,num_threads=1)\n",
    "trainflow.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset = ElePreloader(datafile='data/test_list.csv',batch_size=BATCH_SIZE)\n",
    "with tf.device(\"/gpu:{}\".format(GPU_CORE)):\n",
    "    coord = tf.train.Coordinator()\n",
    "    testflow = FeedDictFlow({\n",
    "            'data':testset,\n",
    "        },coord,batch_size=BATCH_SIZE,shuffle=True,continuous=True,num_threads=1)\n",
    "testflow.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_x1,sample_y1 = trainflow.next()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_x1,sample_y1 = testflow.next()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 10, 9, 14), (128, 2086))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x1.shape,sample_y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c3c4'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(sample_y1[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=uint64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sample_x1[0],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2086"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sample_x1[0],axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_block(inputx,name,training,block_num=2,filters=256,kernel_size=(3,3)):\n",
    "    net = inputx\n",
    "    for i in range(block_num):\n",
    "        net = tf.layers.conv2d(net,filters=filters,kernel_size=kernel_size,activation=None,name=\"{}_res_conv{}\".format(name,i),padding='same')\n",
    "        net = tf.layers.batch_normalization(net,training=training,name=\"{}_res_bn{}\".format(name,i))\n",
    "        if i == block_num - 1:\n",
    "            net = net + inputx #= tf.concat((inputx,net),axis=-1)\n",
    "        net = tf.nn.elu(net,name=\"{}_res_elu{}\".format(name,i))\n",
    "    return net\n",
    "\n",
    "def conv_block(inputx,name,training,block_num=1,filters=2,kernel_size=(1,1)):\n",
    "    net = inputx\n",
    "    for i in range(block_num):\n",
    "        net = tf.layers.conv2d(net,filters=filters,kernel_size=kernel_size,activation=None,name=\"{}_convblock_conv{}\".format(name,i),padding='same')\n",
    "        net = tf.layers.batch_normalization(net,training=training,name=\"{}_convblock_bn{}\".format(name,i))\n",
    "        net = tf.nn.elu(net,name=\"{}_convblock_elu{}\".format(name,i))\n",
    "    # net [None,10,9,2]\n",
    "    netshape = net.get_shape().as_list()\n",
    "    print(\"inside conv block {}\".format(str(netshape)))\n",
    "    net = tf.reshape(net,shape=(-1,netshape[1] * netshape[2] * netshape[3]))\n",
    "    net = tf.layers.dense(net,10 * 9,name=\"{}_dense\".format(name))\n",
    "    net = tf.nn.elu(net,name=\"{}_elu\".format(name))\n",
    "    return net\n",
    "\n",
    "def res_net_board(inputx,name,training,filters=256):\n",
    "    net = inputx\n",
    "    net = tf.layers.conv2d(net,filters=filters,kernel_size=(3,3),activation=None,name=\"{}_res_convb\".format(name),padding='same')\n",
    "    net = tf.layers.batch_normalization(net,training=training,name=\"{}_res_bnb\".format(name))\n",
    "    net = tf.nn.elu(net,name=\"{}_res_elub\".format(name))\n",
    "    for i in range(NUM_RES_LAYERS):\n",
    "        net = res_block(net,name=\"{}_layer_{}\".format(name,i + 1),training=training)\n",
    "        print(net.get_shape().as_list())\n",
    "    print(\"inside res net {}\".format(str(net.get_shape().as_list())))\n",
    "    #net_unsoftmax = conv_block(net,name=\"{}_conv\".format(name),training=training)\n",
    "    return net\n",
    "\n",
    "def get_scatter(name):\n",
    "    with tf.variable_scope(\"Test\"):\n",
    "        ph = tf.placeholder(tf.float32,name=name)\n",
    "        op = tf.summary.scalar(name,ph)\n",
    "    return ph,op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?tf.layers.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "inside res net [None, 10, 9, 256]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "NUM_RES_LAYERS = 5\n",
    "\n",
    "with tf.device(\"/gpu:{}\".format(GPU_CORE)):\n",
    "    X = tf.placeholder(tf.float32,[None,10,9,14])\n",
    "    nextmove = tf.placeholder(tf.float32,[None,len(labels)])\n",
    "    score = tf.placeholder(tf.float32,[None,1])\n",
    "    \n",
    "    training = tf.placeholder(tf.bool,name='training_mode')\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    body = res_net_board(X,\"selectnet\",training=training)\n",
    "    with tf.variable_scope(\"policy_head\"):\n",
    "        policy_head = tf.layers.conv2d(body, 2, 1, padding='SAME')\n",
    "        policy_head = tf.contrib.layers.batch_norm(policy_head, center=False, epsilon=1e-5, fused=True,\n",
    "                                                    is_training=training, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # print(self.policy_head.shape)  # (?, 9, 10, 2)\n",
    "        policy_head = tf.reshape(policy_head, [-1, 9 * 10 * 2])\n",
    "        policy_head = tf.contrib.layers.fully_connected(policy_head, len(labels), activation_fn=None)\n",
    "        #self.policy_head.append(policy_head)    # 保存多个gpu的策略头结果（走子概率向量）\n",
    "\n",
    "    # 价值头\n",
    "    with tf.variable_scope(\"value_head\"):\n",
    "        value_head = tf.layers.conv2d(body, 1, 1, padding='SAME')\n",
    "        value_head = tf.contrib.layers.batch_norm(value_head, center=False, epsilon=1e-5, fused=True,\n",
    "                                        is_training=training, activation_fn=tf.nn.relu)\n",
    "        # print(self.value_head.shape)  # (?, 9, 10, 1)\n",
    "        value_head = tf.reshape(value_head, [-1, 9 * 10 * 1])\n",
    "        value_head = tf.contrib.layers.fully_connected(value_head, 256, activation_fn=tf.nn.relu)\n",
    "        value_head = tf.contrib.layers.fully_connected(value_head, 1, activation_fn=tf.nn.tanh)\n",
    "    \n",
    "    net_unsoftmax = policy_head\n",
    "    \n",
    "\n",
    "    with tf.variable_scope(\"Loss\"):\n",
    "        policy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=nextmove,logits=net_unsoftmax))\n",
    "        #loss_summary = tf.summary.scalar(\"move_loss\",policy_loss)\n",
    "        value_loss = tf.losses.mean_squared_error(labels=score, predictions=value_head) \n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001)\n",
    "        regular_variables = tf.trainable_variables()\n",
    "        l2_loss = tf.contrib.layers.apply_regularization(regularizer, regular_variables)\n",
    "        multitarget_loss = value_loss + policy_loss + l2_loss\n",
    "        \n",
    "    net_softmax = tf.nn.softmax(net_unsoftmax)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(nextmove,1), tf.argmax(net_softmax,1))\n",
    "    \n",
    "    with tf.variable_scope(\"Accuracy\"):\n",
    "        accuracy_select = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "        train_op = optimizer.minimize(policy_loss,global_step=global_step)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.global_step(sess, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: 无法创建目录\"models\": 文件已存在\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"models/{}\".format(model_name)):\n",
    "    os.mkdir(\"models/{}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_BATCH = 10000\n",
    "N_BATCH_TEST = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 STEP 9999 LR 0.1 ACC 26.91 LOSS 2.87  100.00 % [==================================================>] 1280000/1280000 \t used:1447s eta:0 ss s\n",
      " epoch 1 100.00 % [==================================================>] 320000/320000 \t used:233s eta:0 sEPOCH 1 valid loss 2.881636381149292 acc 26.750625\n",
      "\n",
      "EPOCH 2 STEP 9999 LR 0.1 ACC 30.91 LOSS 2.61  100.00 % [==================================================>] 1280000/1280000 \t used:1430s eta:0 ss\n",
      " epoch 2 100.00 % [==================================================>] 320000/320000 \t used:243s eta:0 sEPOCH 2 valid loss 2.6430976390838623 acc 30.1371875\n",
      "\n",
      "EPOCH 3 STEP 9999 LR 0.1 ACC 33.84 LOSS 2.46  100.00 % [==================================================>] 1280000/1280000 \t used:1428s eta:0 ss\n",
      " epoch 3 100.00 % [==================================================>] 320000/320000 \t used:242s eta:0 sEPOCH 3 valid loss 2.526669502258301 acc 32.25875\n",
      "\n",
      "EPOCH 4 STEP 9999 LR 0.1 ACC 33.25 LOSS 2.44  100.00 % [==================================================>] 1280000/1280000 \t used:1428s eta:0 ss\n",
      " epoch 4 100.00 % [==================================================>] 320000/320000 \t used:244s eta:0 sEPOCH 4 valid loss 2.4308414459228516 acc 34.370625\n",
      "\n",
      "EPOCH 5 STEP 9999 LR 0.1 ACC 35.22 LOSS 2.36  100.00 % [==================================================>] 1280000/1280000 \t used:1434s eta:0 ss\n",
      " epoch 5 100.00 % [==================================================>] 320000/320000 \t used:248s eta:0 sEPOCH 5 valid loss 2.3341262340545654 acc 35.778125\n",
      "\n",
      "EPOCH 6 STEP 9999 LR 0.1 ACC 37.55 LOSS 2.25  100.00 % [==================================================>] 1280000/1280000 \t used:1438s eta:0 ss\n",
      " epoch 6 100.00 % [==================================================>] 320000/320000 \t used:241s eta:0 sEPOCH 6 valid loss 2.2709171772003174 acc 36.915625\n",
      "\n",
      "EPOCH 7 STEP 9999 LR 0.1 ACC 40.51 LOSS 2.11  100.00 % [==================================================>] 1280000/1280000 \t used:1421s eta:0 ss\n",
      " epoch 7 100.00 % [==================================================>] 320000/320000 \t used:243s eta:0 sEPOCH 7 valid loss 2.225374221801758 acc 38.028125\n",
      "\n",
      "EPOCH 8 STEP 9999 LR 0.1 ACC 39.73 LOSS 2.12  100.00 % [==================================================>] 1280000/1280000 \t used:1685s eta:0 ss\n",
      " epoch 8 100.00 % [==================================================>] 320000/320000 \t used:491s eta:0 sEPOCH 8 valid loss 2.1745471954345703 acc 39.2059375\n",
      "\n",
      "EPOCH 9 STEP 9999 LR 0.1 ACC 42.3 LOSS 2.02  100.00 % [==================================================>] 1280000/1280000 \t used:1695s eta:0 sss\n",
      " epoch 9 100.00 % [==================================================>] 320000/320000 \t used:514s eta:0 sEPOCH 9 valid loss 2.121812105178833 acc 39.8625\n",
      "\n",
      "EPOCH 10 STEP 3706 LR 0.1 ACC 43.65 LOSS 1.97  37.07 % [==================>--------------------------------] 474496/1280000 \t used:638s eta:1084 s"
     ]
    }
   ],
   "source": [
    "restore = False\n",
    "N_EPOCH = 100\n",
    "DECAY_EPOCH = 20\n",
    "\n",
    "class ExpVal:\n",
    "    def __init__(self,exp_a=0.97):\n",
    "        self.val = None\n",
    "        self.exp_a = exp_a\n",
    "    def update(self,newval):\n",
    "        if self.val == None:\n",
    "            self.val = newval\n",
    "        else:\n",
    "            self.val = self.exp_a * self.val + (1 - self.exp_a) * newval\n",
    "    def getval(self):\n",
    "        return round(self.val,2)\n",
    "    \n",
    "expacc_move = ExpVal()\n",
    "exploss = ExpVal()\n",
    "\n",
    "\n",
    "begining_learning_rate = 1e-1\n",
    "\n",
    "pred_image = None\n",
    "if restore == False:\n",
    "    train_epoch = 1\n",
    "    train_batch = 0\n",
    "for one_epoch in range(train_epoch,N_EPOCH):\n",
    "    train_epoch = one_epoch\n",
    "    pb = ProgressBar(worksum=N_BATCH * BATCH_SIZE,info=\" epoch {} batch {}\".format(train_epoch,train_batch))\n",
    "    pb.startjob()\n",
    "    \n",
    "    for one_batch in range(N_BATCH):\n",
    "        if restore == True and one_batch < train_batch:\n",
    "            pb.auto_display = False\n",
    "            pb.complete(BATCH_SIZE)\n",
    "            pb.auto_display = True\n",
    "            continue\n",
    "        else:\n",
    "            restore = False\n",
    "        train_batch = one_batch\n",
    "        \n",
    "        batch_x,batch_y = trainflow.next()['data']\n",
    "        # learning rate decay strategy\n",
    "        batch_lr = begining_learning_rate * 10 ** -(one_epoch // DECAY_EPOCH)\n",
    "        \n",
    "        _,step_loss,step_acc_move,step_value = sess.run(\n",
    "            [train_op,policy_loss,accuracy_select,global_step],feed_dict={\n",
    "                X:batch_x,nextmove:batch_y,learning_rate:batch_lr,training:True\n",
    "            })\n",
    "        \n",
    "        step_acc_move *= 100\n",
    "        \n",
    "        expacc_move.update(step_acc_move)\n",
    "        exploss.update(step_loss)\n",
    "\n",
    "       \n",
    "        pb.info = \"EPOCH {} STEP {} LR {} ACC {} LOSS {} \".format(\n",
    "            one_epoch,one_batch,batch_lr,expacc_move.getval(),exploss.getval())\n",
    "        \n",
    "        pb.complete(BATCH_SIZE)\n",
    "    print()\n",
    "    pb = ProgressBar(worksum=N_BATCH // 4 * BATCH_SIZE,info=\" epoch {}\".format(train_epoch))\n",
    "    pb.startjob()\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for one_batch in range(N_BATCH // 4):\n",
    "        batch_x,batch_y = testflow.next()['data']\n",
    "        # learning rate decay strategy\n",
    "        batch_lr = begining_learning_rate * 10 ** -(one_epoch // DECAY_EPOCH)\n",
    "        \n",
    "        step_loss,step_acc_move,step_value = sess.run(\n",
    "            [policy_loss,accuracy_select,global_step],feed_dict={\n",
    "                X:batch_x,nextmove:batch_y,learning_rate:batch_lr,training:False\n",
    "            })\n",
    "        \n",
    "        step_acc_move *= 100\n",
    "        losses.append(step_loss)\n",
    "        accs.append(step_acc_move)\n",
    "        pb.complete(BATCH_SIZE)\n",
    "    print(\"EPOCH {} valid loss {} acc {}\".format(train_epoch,np.average(losses),np.average(accs)))\n",
    "    print()\n",
    "    saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "    saver.save(sess,\"models/{}/model_{}\".format(model_name,one_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tf1.3_python",
   "language": "python",
   "name": "tf1.3_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
